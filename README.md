# üõí E-Commerce Analytics Platform

> Enterprise-grade data engineering solution built on Azure Databricks with Unity Catalog

[![Databricks](https://img.shields.io/badge/Databricks-FF3621?style=for-the-badge&logo=databricks&logoColor=white)](https://databricks.com)
[![Azure](https://img.shields.io/badge/Azure-0089D6?style=for-the-badge&logo=microsoft-azure&logoColor=white)](https://azure.microsoft.com)
[![Delta Lake](https://img.shields.io/badge/Delta_Lake-00ADD8?style=for-the-badge&logo=delta&logoColor=white)](https://delta.io)

## üìã Overview

This platform processes e-commerce customer behavior data (~110M events) through a **Medallion Architecture** (Bronze ‚Üí Silver ‚Üí Gold) using:

- **Unity Catalog** for data governance
- **Delta Live Tables** for declarative pipelines
- **Databricks Asset Bundles** for CI/CD deployment
- **Structured Streaming** with Auto Loader for incremental ingestion

## üèóÔ∏è Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                        Unity Catalog                            ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  ecommerce_analytics_{env}/                                     ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ bronze_layer/                                              ‚îÇ
‚îÇ  ‚îÇ   ‚îú‚îÄ‚îÄ events_raw (Delta Table)                              ‚îÇ
‚îÇ  ‚îÇ   ‚îî‚îÄ‚îÄ raw_data (Volume - CSVs)                              ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ silver_layer/                                              ‚îÇ
‚îÇ  ‚îÇ   ‚îú‚îÄ‚îÄ events_cleaned                                         ‚îÇ
‚îÇ  ‚îÇ   ‚îú‚îÄ‚îÄ users_dim                                              ‚îÇ
‚îÇ  ‚îÇ   ‚îî‚îÄ‚îÄ products_dim                                           ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ gold_layer/                                                ‚îÇ
‚îÇ      ‚îú‚îÄ‚îÄ customer_metrics                                       ‚îÇ
‚îÇ      ‚îú‚îÄ‚îÄ product_performance                                    ‚îÇ
‚îÇ      ‚îú‚îÄ‚îÄ daily_sales_summary                                    ‚îÇ
‚îÇ      ‚îî‚îÄ‚îÄ conversion_funnel                                      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## üìÅ Project Structure

```
azure-databricks-ecommerce-analytics/
‚îú‚îÄ‚îÄ databricks.yml              # DABs main configuration
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ environments/               # Environment-specific configs
‚îÇ   ‚îú‚îÄ‚îÄ dev.yml
‚îÇ   ‚îú‚îÄ‚îÄ staging.yml
‚îÇ   ‚îî‚îÄ‚îÄ prod.yml
‚îú‚îÄ‚îÄ resources/                  # DABs resource definitions
‚îÇ   ‚îú‚îÄ‚îÄ jobs.yml
‚îÇ   ‚îú‚îÄ‚îÄ pipelines.yml
‚îÇ   ‚îî‚îÄ‚îÄ clusters.yml
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ bronze/                 # Bronze layer notebooks
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ingest_events_csv.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ schema_events.py
‚îÇ   ‚îú‚îÄ‚îÄ silver/                 # Silver layer notebooks
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ transform_events_cleaned.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ data_quality_rules.py
‚îÇ   ‚îú‚îÄ‚îÄ gold/                   # Gold layer notebooks
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ agg_customer_metrics.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ agg_product_performance.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ agg_daily_sales.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ agg_conversion_funnel.py
‚îÇ   ‚îú‚îÄ‚îÄ dlt/                    # Delta Live Tables
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ dlt_bronze_to_gold.py
‚îÇ   ‚îî‚îÄ‚îÄ streaming/              # Streaming pipelines
‚îÇ       ‚îî‚îÄ‚îÄ stream_events_autoloader.py
‚îú‚îÄ‚îÄ setup/                      # Unity Catalog setup scripts
‚îÇ   ‚îú‚îÄ‚îÄ unity_catalog_setup.sql
‚îÇ   ‚îú‚îÄ‚îÄ external_locations.sql
‚îÇ   ‚îú‚îÄ‚îÄ security_policies.sql
‚îÇ   ‚îî‚îÄ‚îÄ compute_policies.sql
‚îú‚îÄ‚îÄ tests/                      # Unit tests
‚îÇ   ‚îú‚îÄ‚îÄ test_bronze_ingestion.py
‚îÇ   ‚îú‚îÄ‚îÄ test_silver_transformations.py
‚îÇ   ‚îî‚îÄ‚îÄ test_gold_aggregations.py
‚îú‚îÄ‚îÄ docs/                       # Documentation
‚îÇ   ‚îú‚îÄ‚îÄ architecture.md
‚îÇ   ‚îú‚îÄ‚îÄ medallion_design.md
‚îÇ   ‚îú‚îÄ‚îÄ security_model.md
‚îÇ   ‚îú‚îÄ‚îÄ performance_optimizations.md
‚îÇ   ‚îî‚îÄ‚îÄ demo_walkthrough.md
‚îî‚îÄ‚îÄ .github/
    ‚îî‚îÄ‚îÄ workflows/
        ‚îî‚îÄ‚îÄ deploy.yml          # CI/CD pipeline
```

## üöÄ Quick Start (Free Trial / Serverless)

> **‚úÖ Serverless Compatible**: This project is configured to run on Databricks Free Trial with serverless compute.

### Prerequisites

- Azure Databricks **Free Trial** workspace
- Unity Catalog enabled (automatic on new workspaces)
- Databricks CLI v0.200+

### 1. Download Dataset

First, download the dataset from Kaggle and upload to your workspace:

1. Go to [Kaggle Dataset](https://www.kaggle.com/datasets/mkechinov/ecommerce-behavior-data-from-multi-category-store)
2. Download CSV files (~14GB)
3. Create Unity Catalog volume and upload (or use included notebook)

### 2. Create Serverless SQL Warehouse

1. In Databricks UI, go to **SQL Warehouses**
2. Click **Create SQL Warehouse**
3. Select **Serverless** type
4. Name: `ecommerce-analytics-warehouse`
5. Copy the **Warehouse ID** for configuration

### 3. Configure CLI

```bash
# Install Databricks CLI
pip install databricks-cli

# Configure with your workspace
databricks configure --token
# Enter your workspace URL and Personal Access Token
```

### 4. Update Configuration

Edit `databricks.yml` and set your workspace URL:

```yaml
variables:
  databricks_host:
    default: "https://adb-xxxxx.azuredatabricks.net"
  warehouse_id:
    default: "your-warehouse-id-here"
```

### 5. Validate & Deploy

```bash
# Validate configuration
databricks bundle validate -t dev

# Deploy to workspace
databricks bundle deploy -t dev
```

### 6. Run Pipelines

```bash
# Option 1: Run batch ingestion job
databricks bundle run -t dev bronze_ingestion_job

# Option 2: Run full pipeline (Bronze ‚Üí Silver ‚Üí Gold)
databricks bundle run -t dev full_pipeline_job

# Option 3: Run DLT pipeline (serverless)
databricks bundle run -t dev ecommerce_dlt_pipeline
```

### 7. Interactive Notebooks (Alternative)

For Free Trial, you can also run notebooks directly:

1. Navigate to Workspace in Databricks UI
2. Open notebooks from deployed bundle
3. Attach to **Serverless** compute
4. Run cells interactively

## üìä Dataset

**Source**: [Kaggle - eCommerce Behavior Data](https://www.kaggle.com/datasets/mkechinov/ecommerce-behavior-data-from-multi-category-store)

| Metric       | Value                                  |
| ------------ | -------------------------------------- |
| Total Events | ~110 million                           |
| Time Period  | October - November 2019                |
| File Size    | ~14 GB (CSV)                           |
| Event Types  | view, cart, remove_from_cart, purchase |

### Schema

| Column        | Type      | Description                         |
| ------------- | --------- | ----------------------------------- |
| event_time    | timestamp | Event timestamp (UTC)               |
| event_type    | string    | view/cart/remove_from_cart/purchase |
| product_id    | long      | Product identifier                  |
| category_id   | long      | Category identifier                 |
| category_code | string    | Category taxonomy (nullable)        |
| brand         | string    | Brand name (nullable)               |
| price         | double    | Product price                       |
| user_id       | long      | User identifier                     |
| user_session  | string    | Session identifier                  |

## üîê Security Features

- **Row Level Security (RLS)**: Data isolation by user segment
- **Column Level Security (CLS)**: PII protection for sensitive columns
- **Dynamic Data Masking**: Real-time masking based on user roles
- **Audit Logging**: Complete access trail via Unity Catalog

## üìà Key Metrics (Gold Layer)

### Customer Metrics

- Customer Lifetime Value (CLV)
- Session counts and engagement
- Purchase frequency
- Churn indicators

### Product Performance

- Conversion funnel (View ‚Üí Cart ‚Üí Purchase)
- Revenue by product/category/brand
- Top performing products

### Daily Sales Summary

- Revenue trends
- Order counts
- Average order value
- Peak shopping hours

## üß™ Testing

```bash
# Run all tests
pytest tests/ -v

# Run specific test file
pytest tests/test_silver_transformations.py -v
```

## üìö Documentation

- [Architecture Overview](docs/architecture.md)
- [Medallion Design](docs/medallion_design.md)
- [Security Model](docs/security_model.md)
- [Performance Optimizations](docs/performance_optimizations.md)
- [Demo Walkthrough](docs/demo_walkthrough.md)

## ü§ù Contributing

1. Create a feature branch from `main`
2. Make changes and test locally
3. Submit a pull request
4. CI/CD will validate and deploy to dev

## üìÑ License

This project is for educational/assessment purposes.

---

**Author**: Built for Azure Databricks Associate Practical Assessment  
**Dataset Credit**: [Michael Kechinov](https://www.kaggle.com/mkechinov)
